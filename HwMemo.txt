ex1:

Compute cost:


The objective of linear regression this time is to minimize the cost function
J as linear regression, each ans goes to visualization. theta coming from gradient descent.


gradient descent:

theta  = like an array with [0;0]

start putting data into it.
since Octave array starts from ONE, the index for this array will start from 1.

alpha is learning rate

X = data(:, 1); y = data(:, 2);


  %video : Gradient Descent 9:00: correct: simultaneous update
    % this time from 1, not 0
    % sum is the best way to avoid loop
    % theta are all column vector with 2 elements
    % .* when two matrices element 
    %   http://www.glue.umd.edu/afs/glue.umd.edu/system/info/olh/Numerical/Matlab_Matrix_Manipulation_Software/Matrix_Vector_Operations/elementwise
	for more matrix example.


element wise operation: rather doing n




ex2


Cost function:
	== smilar to linear regression 
but if we implement logistic regression as linear regression, it will be convex since sigmoid function is different.


1.2.2 Cost function and gradient

if y  =  1
 -log(hypo(x))

if y  = 0
 -log(1-hypo(x))



** in ex1, hypo(x) = X*theta
 but this time, using sigmoid, hypo(x) => sigmoid.
we need to pass hypo(x) to sigmoid 




Logistic regression
y = 0 r 1 always

cost = -ylog(hypo(x)) - (1-y)log(1-hypo(x))

definition of hypo is different here
hypo(x) = 1 / 1+e^-(root^t)*x


Multiclass classification

one vs all

classify for each case




Overfitting
:undercutting, high bias
:overfitting, high various â€” too many hypo. fail
















